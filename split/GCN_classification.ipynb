{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 目前思路：GNN等方法可作为规则挖掘方法的补充验证，另外可调研GNN可解释性算法（近期研究），用于后续补充解释判别原因\n",
    "\n",
    "* 点的属性定义\n",
    "  * initial_deposit(初始账户金额)、prior_sar_count(用户违约标签)\n",
    "  * 根据边属性添加的属性：\n",
    "    * 账户转入总金额\n",
    "    * 账户转出总金额\n",
    "    * 账户总转入次数\n",
    "    * 账户总转出次数\n",
    "    * 大额交易次数 （目前定义为总交易数据的前10%）\n",
    "* 后续需要扩充的属性：时间维度（涉及到时序GNN的处理以及动态图的规则挖掘）\n",
    "  * 需要讨论的问题\n",
    "    * 是否需要将交易图按照交易时间窗口动态划分多个子图进行挖掘？（或者直接将交易时间作为边属性挖掘整个图）\n",
    "    * 边标签与点标签的交互问题（是否直接将边标签作为边的一个属性特征：带边属性的GNN学习）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入属性的归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "#编号\n",
    "index = 3\n",
    "\n",
    "ac_df = pd.read_csv('data/updated_accounts.csv')\n",
    "\n",
    "# 定义需要输入模型的属性\n",
    "features_to_input = ['acct_id','prior_sar_count','initial_deposit','total_outgoing_amount', 'total_incoming_amount', \n",
    "                         'outgoing_transactions', 'incoming_transactions',\n",
    "                         'high_value_outgoing_transactions', 'high_value_incoming_transactions']\n",
    "\n",
    "# 定义需要归一化的列\n",
    "features_to_normalize = ['initial_deposit','total_outgoing_amount', 'total_incoming_amount', \n",
    "                         'outgoing_transactions', 'incoming_transactions',\n",
    "                         'high_value_outgoing_transactions', 'high_value_incoming_transactions']\n",
    "\n",
    "# 仅选取需要归一化的列\n",
    "ac_df_subset = ac_df[features_to_normalize]\n",
    "\n",
    "# 初始化MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 归一化这些列\n",
    "ac_df_normalized = pd.DataFrame(scaler.fit_transform(ac_df_subset), columns=features_to_normalize)\n",
    "\n",
    "# 将归一化后的数据列加入到原始DataFrame中（替换原始列）\n",
    "ac_df.update(ac_df_normalized)\n",
    "# 将 'prior_sar' 转换为数值型标签，假设它是一个布尔型列\n",
    "ac_df['prior_sar_count'] = ac_df['prior_sar_count'].astype(int)\n",
    "\n",
    "ac_df_subset1 = ac_df[features_to_input]\n",
    "# # 如果需要保存DataFrame到CSV文件\n",
    "# ac_df_subset.to_csv('/mnt/data/normalized_accounts.csv', index=False)\n",
    "\n",
    "# 显示归一化后的前几行数据进行检查\n",
    "ac_df_subset1.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建geometric数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm #进度条\n",
    "\n",
    "# 提取特征列作为x\n",
    "features = ac_df_subset1.drop(['acct_id', 'prior_sar_count'], axis=1)\n",
    "\n",
    "x = torch.tensor(features.values, dtype=torch.float)\n",
    "\n",
    "# 提取'prior_sar_count'列作为y\n",
    "y = torch.tensor(ac_df_subset1['prior_sar_count'].values, dtype=torch.long)\n",
    "\n",
    "print(\"First 10 x:\", x[:10])\n",
    "print(\"First 10 y:\", y[:10])\n",
    "\n",
    "trans_df = pd.read_csv('data/transactions_'+str(index)+'.csv')\n",
    "source_nodes = trans_df.orig_acct\n",
    "target_nodes = trans_df.bene_acct\n",
    "edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "print(edge_index)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "print('data:',data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对比试验：用传统的全连接层(Multi-layer Perception Network)\n",
    "##### 即点和点之间相互独立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(7, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model = MLP(hidden_channels=16)\n",
    "\n",
    "# 统计每个类别的样本数\n",
    "labels = data.y\n",
    "class_counts = Counter(labels.numpy())\n",
    "# 输出样本类别数目\n",
    "print(class_counts)\n",
    "class_weights = {class_id: 1.0 / count for class_id, count in class_counts.items()}\n",
    "# 将权重转换为张量\n",
    "weights = torch.tensor([class_weights[i] for i in range(len(class_counts))], dtype=torch.float)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights) #损失函数加入权重\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "# 计算分割点，90%用于训练\n",
    "num_nodes = data.y.size(0) # 总节点数\n",
    "train_split = int(num_nodes * 0.9) # 前90%的节点数\n",
    "\n",
    "# 创建训练和测试掩码\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[:train_split] = True\n",
    "test_mask[train_split:] = True\n",
    "\n",
    "def predict_with_threshold(out, threshold):\n",
    "    # 对概率分布进行预测，若类别 1 的概率高于阈值则标记为正类\n",
    "    return (out[:, 1] >= threshold).int()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x[train_mask])  # Perform a single forward pass.\n",
    "    loss = criterion(out, data.y[train_mask])  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x[test_mask])\n",
    "    print(out)\n",
    "    probs = torch.softmax(out, dim=1)[:, 1]  # 计算类别为1的概率\n",
    "    # pred = out.argmax(dim=1)  # 选择概率最高的作为分类类别\n",
    "    pred = torch.softmax(out, dim=1)\n",
    "    pred = predict_with_threshold(pred,0.4544)\n",
    "    print(pred)\n",
    "    count = torch.sum(out.argmax(dim=1) == 1).item()\n",
    "    print(count)\n",
    "\n",
    "    # F1值\n",
    "    # 针对指定类别计算 F1 分数，需确保 `true_labels` 和 `pred` 中包含该类别\n",
    "    target_class = 1\n",
    "    f1 = f1_score(data.y[test_mask].cpu(), pred.cpu(), labels=[target_class], average='binary')\n",
    "    precision = precision_score(data.y[test_mask].cpu(), pred.cpu(), labels=[target_class], average='binary')\n",
    "    recall = recall_score(data.y[test_mask].cpu(), pred.cpu(), labels=[target_class], average='binary')\n",
    "    print(f'Precision (Class {target_class}): {precision:.4f}')\n",
    "    print(f'Recall (Class {target_class}): {recall:.4f}')\n",
    "    print(f'F1 Score (Class {target_class}): {f1:.4f}')\n",
    "    return f1,probs.detach().cpu().numpy()\n",
    "    # #直接计算acc的方式\n",
    "    # test_correct = pred == data.y[test_mask]  # Check against ground-truth labels.\n",
    "    # test_acc = int(test_correct.sum()) / int(test_mask.sum())  # Derive ratio of correct predictions.\n",
    "    # return test_acc\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_probs = []  # 用于存储所有概率值\n",
    "test_acc,probs = test()\n",
    "\n",
    "# test_acc = test()\n",
    "\n",
    "all_probs.extend(probs)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# 将概率值转换为 NumPy 数组\n",
    "all_probs_np = np.array(all_probs)\n",
    "\n",
    "# 找到第90百分位的概率(原数据异常占比大概6%)\n",
    "percentile_90 = np.percentile(all_probs_np, 90)\n",
    "print(f'90th Percentile Probability: {percentile_90:.4f}')\n",
    "\n",
    "# 绘制概率分布图\n",
    "plt.hist(all_probs_np, bins=30, alpha=0.75, color='blue')\n",
    "plt.axvline(percentile_90, color='red', linestyle='dashed', linewidth=2, label='90th Percentile')\n",
    "plt.title('Probability Distribution of Class 1')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Network (GNN)\n",
    "\n",
    "将全连接层替换成GCN层\n",
    "* 目前使用的GNN未考虑边的属性值（下一步加入边属性更新的GNN）\n",
    "* 样本不平衡解决方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(7, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "    # 在GCN过程中图结构即edge_index没有发生改变\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights) #加入权重\n",
    "\n",
    "\n",
    "def count_substrings_in_file(file_path, substring):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        count = content.count(substring)\n",
    "        \n",
    "        return count\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "def save_model_txt(model, scale_factor, filename):\n",
    "    \"\"\"\n",
    "    将PyTorch模型的参数乘以指定的因子并保存到txt文件中。\n",
    "    \n",
    "    参数:\n",
    "    model (torch.nn.Module): PyTorch模型。\n",
    "    scale_factor (float): 乘法因子。\n",
    "    filename (str): 保存的文件名。\n",
    "    \"\"\"\n",
    "    # 模型参数\n",
    "    params = model.state_dict()\n",
    "\n",
    "    # 将所有参数展开成一维向量并乘以较大的数\n",
    "    all_params = []\n",
    "    for value in params.values():\n",
    "        flattened_value =(value.view(-1).numpy() * scale_factor)  # 将参数展开成一维向量并乘以较大的数\n",
    "        \n",
    "        rounded_value = np.round(flattened_value).astype(int)  # 取整\n",
    "        all_params.extend(rounded_value)\n",
    "        \n",
    "        # all_params.extend(flattened_value)\n",
    "\n",
    "    all_params_number = len(all_params)\n",
    "    \n",
    "    \n",
    "    # 将所有参数保存到txt文件\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(str(all_params_number)+'\\n')\n",
    "        f.write(' '.join(map(str, all_params)) + '\\n')\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  \n",
    "    out = model(data.x, data.edge_index)  \n",
    "    loss = criterion(out, data.y)  \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    save_model_txt(model, scale_factor=1e10, filename='GCN_params'+'_'+str(index)+'.txt')  \n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    probs = torch.softmax(out, dim=1)[:, 1]  # 计算类别为1的概率\n",
    "    # pred = out.argmax(dim=1)\n",
    "    pred = torch.softmax(out, dim=1)\n",
    "    pred = predict_with_threshold(pred,0.62)\n",
    "    # count = torch.sum(out.argmax(dim=1) == 1).item()\n",
    "    # print(count)\n",
    "    \n",
    "    # F1值\n",
    "    # 针对指定类别计算 F1 分数，需确保 `true_labels` 和 `pred` 中包含该类别\n",
    "    target_class = 1\n",
    "    f1 = f1_score(data.y.cpu(), pred.cpu(), labels=[target_class], average='binary')\n",
    "    precision = precision_score(data.y.cpu(), pred.cpu(), labels=[target_class], average='binary')\n",
    "    recall = recall_score(data.y.cpu(), pred.cpu(), labels=[target_class], average='binary')\n",
    "    print(f'Precision (Class {target_class}): {precision:.4f}')\n",
    "    print(f'Recall (Class {target_class}): {recall:.4f}')\n",
    "    print(f'F1 Score (Class {target_class}): {f1:.4f}')\n",
    "    return f1,probs.detach().cpu().numpy()\n",
    "\n",
    "    # test_correct = pred == data.y \n",
    "    # test_acc = int(test_correct.sum()) / int(data.x.size(0))  \n",
    "    # return test_acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_probs = []  # 用于存储所有概率值\n",
    "test_acc,probs = test()\n",
    "# test_acc = test()\n",
    "\n",
    "all_probs.extend(probs)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# 将概率值转换为 NumPy 数组\n",
    "all_probs_np = np.array(all_probs)\n",
    "\n",
    "# 找到第90百分位的概率(原数据异常占比大概6%)\n",
    "percentile_90 = np.percentile(all_probs_np, 90)\n",
    "print(f'90th Percentile Probability: {percentile_90:.4f}')\n",
    "\n",
    "# 绘制概率分布图\n",
    "plt.hist(all_probs_np, bins=30, alpha=0.75, color='blue')\n",
    "plt.axvline(percentile_90, color='red', linestyle='dashed', linewidth=2, label='90th Percentile')\n",
    "plt.title('Probability Distribution of Class 1')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 存在问题，样本不均衡导致判别过程的问题？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
